{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cfd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607cf1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:9000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"AWS_ENDPOINT_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df204ef",
   "metadata": {},
   "source": [
    "## Dataset Info in MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e573a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_uri = \"s3://dataset/win_assist_dataset/v1.0/mistral_inst_format.json\"\n",
    "dataset_version = \"v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9e0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"json\", data_files=train_dataset_uri)[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464aa53",
   "metadata": {},
   "source": [
    "## Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3367ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b12c8fb974e4a0dbbf7fcf0cd13e367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4fd6c",
   "metadata": {},
   "source": [
    "## LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f56f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078190a6",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20186c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_mistral_checkpoint\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"mlflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131123b",
   "metadata": {},
   "source": [
    "## Run MLflow exp run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eebdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/29 23:31:06 INFO mlflow.tracking.fluent: Experiment with name 'win_assistant_mistral7b_inst' does not exist. Creating a new experiment.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\Public\\DipeshYogi\\envs\\win_assistant\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.579600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run mistral7b_lora_finetune_v2 at: http://localhost:5000/#/experiments/1/runs/72970c479b3344df9df0c0111ddcdc38\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"win_assistant_mistral7b_inst\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"mistral7b_lora_finetune_v2\"):\n",
    "    # Log dataset info\n",
    "    mlflow.log_param(\"train_dataset_uri\", train_dataset_uri)\n",
    "    mlflow.log_param(\"train_dataset_version\", dataset_version)\n",
    "\n",
    "    # Log model + LoRA hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": model_name,\n",
    "        \"lora_r\": peft_config.r,\n",
    "        \"lora_alpha\": peft_config.lora_alpha,\n",
    "        \"lora_dropout\": peft_config.lora_dropout,\n",
    "        \"target_modules\": \",\".join(peft_config.target_modules),\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"fp16\": training_args.fp16\n",
    "    })\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,    \n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=peft_config,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986c115",
   "metadata": {},
   "source": [
    "## Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0738c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "2025/09/29 23:36:22 INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\n",
      "2025/09/29 23:36:23 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained argumentis set to False. The reference to the HuggingFace Hub repository mistralai/Mistral-7B-Instruct-v0.1 will be logged instead.\n",
      "2025/09/29 23:36:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/29 23:36:24 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.22.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.22.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/29 23:36:24 INFO mlflow.transformers: A local checkpoint path or PEFT model is given as the `transformers_model`. To avoid loading the full model into memory, we don't infer the pip requirement for the model. Instead, we will use the default requirements, but it may not capture all required pip libraries for the model. Consider providing the pip requirements explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run mistral7b_lora_finetune_v2 at: http://localhost:5000/#/experiments/1/runs/72970c479b3344df9df0c0111ddcdc38\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "last_run_id = mlflow.last_active_run().info.run_id\n",
    "\n",
    "tokenizer_no_pad = AutoTokenizer.from_pretrained(model_name, add_bos_token=True)\n",
    "\n",
    "with mlflow.start_run(run_id=last_run_id):\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer_no_pad},\n",
    "        name=\"win_assist_model\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "win_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
